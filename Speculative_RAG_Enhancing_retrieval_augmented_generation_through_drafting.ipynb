{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlJmLthJsdeZNnxdaKhded",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walkerjian/dev/blob/main/Speculative_RAG_Enhancing_retrieval_augmented_generation_through_drafting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the **Speculative Retrieval-Augmented Generation (Speculative RAG)** in a platform like Google Colab, follow these steps:\n",
        "\n",
        "### 1. **Set Up the Environment**\n",
        "   - Start with setting up your environment, ensuring all necessary libraries and frameworks are installed.\n",
        "\n",
        "   ```python\n",
        "   !pip install torch transformers sentence-transformers scikit-learn\n",
        "   ```\n",
        "\n",
        "### 2. **Load Pre-trained Models**\n",
        "   - You need two models: a smaller specialist LM (`MDrafter`) and a larger generalist LM (`MVerifier`). You can use models from Hugging Face.\n",
        "\n",
        "   ```python\n",
        "   from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "   # Load the drafter model (small specialist model)\n",
        "   drafter_model_name = \"facebook/bart-large\"\n",
        "   drafter_model = AutoModelForSeq2SeqLM.from_pretrained(drafter_model_name)\n",
        "   drafter_tokenizer = AutoTokenizer.from_pretrained(drafter_model_name)\n",
        "\n",
        "   # Load the verifier model (larger generalist model)\n",
        "   verifier_model_name = \"google/t5-large-lm-adapt\"\n",
        "   verifier_model = AutoModelForSeq2SeqLM.from_pretrained(verifier_model_name)\n",
        "   verifier_tokenizer = AutoTokenizer.from_pretrained(verifier_model_name)\n",
        "   ```\n",
        "\n",
        "### 3. **Document Retrieval and Clustering**\n",
        "   - Retrieve relevant documents and cluster them using an embedding model like `sentence-transformers`.\n",
        "\n",
        "   ```python\n",
        "   from sentence_transformers import SentenceTransformer\n",
        "   from sklearn.cluster import KMeans\n",
        "\n",
        "   embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "   # Assume `retrieved_docs` is a list of retrieved documents\n",
        "   doc_embeddings = embedding_model.encode(retrieved_docs)\n",
        "\n",
        "   # Cluster the documents\n",
        "   num_clusters = 5  # This can be tuned\n",
        "   kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "   kmeans.fit(doc_embeddings)\n",
        "   clusters = kmeans.labels_\n",
        "\n",
        "   # Create subsets from each cluster\n",
        "   doc_subsets = []\n",
        "   for i in range(num_clusters):\n",
        "       cluster_docs = [retrieved_docs[j] for j in range(len(retrieved_docs)) if clusters[j] == i]\n",
        "       doc_subsets.append(cluster_docs)\n",
        "   ```\n",
        "\n",
        "### 4. **Draft Generation with MDrafter**\n",
        "   - For each subset of documents, generate drafts using the drafter model.\n",
        "\n",
        "   ```python\n",
        "   drafts = []\n",
        "   for subset in doc_subsets:\n",
        "       input_text = \" \".join(subset)\n",
        "       inputs = drafter_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "       outputs = drafter_model.generate(**inputs)\n",
        "       draft = drafter_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "       drafts.append(draft)\n",
        "   ```\n",
        "\n",
        "### 5. **Verification with MVerifier**\n",
        "   - Use the verifier model to evaluate each draft and select the most reliable one.\n",
        "\n",
        "   ```python\n",
        "   best_draft = None\n",
        "   best_score = float('-inf')\n",
        "\n",
        "   for draft in drafts:\n",
        "       verifier_input = verifier_tokenizer(draft, return_tensors=\"pt\")\n",
        "       verifier_output = verifier_model.generate(**verifier_input)\n",
        "       score = verifier_model(**verifier_input, labels=verifier_input[\"input_ids\"]).loss.item()\n",
        "       \n",
        "       if score > best_score:\n",
        "           best_draft = draft\n",
        "           best_score = score\n",
        "   ```\n",
        "\n",
        "### 6. **Final Answer Selection**\n",
        "   - The best draft with the highest score is selected as the final answer.\n",
        "\n",
        "   ```python\n",
        "   print(\"Best Draft:\", best_draft)\n",
        "   ```\n",
        "\n",
        "### 7. **Optimizations and Scaling**\n",
        "   - Implement parallel processing or multi-threading to handle larger datasets and speed up the process, particularly during the drafting stage.\n",
        "   - Consider using Tensor Parallelism if working with extremely large models to distribute the computational load across multiple GPUs.\n",
        "\n",
        "### 8. **Further Customization**\n",
        "   - You may modify the scoring mechanism to include more sophisticated metrics or incorporate additional model layers for enhanced reasoning capabilities.\n",
        "   - Fine-tune the `MDrafter` model for specific types of queries or integrate specialized token-level adjustments for better rationale generation.\n",
        "\n",
        "### 9. **Testing and Validation**\n",
        "   - Test the entire pipeline on various datasets like TriviaQA, PubHealth, etc., to validate performance improvements in both accuracy and latency.\n",
        "\n",
        "This setup in Google Colab should allow you to effectively implement the Speculative RAG framework. Adjustments might be necessary based on the specific dataset or the computational resources available."
      ],
      "metadata": {
        "id": "wgAAfaBp9wAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##i like this, and will implement it, but i also want a real world example to sink its' teeth into, such as how to generate essays that are undetectable by any ai detector, have a think abut that\n",
        "\n"
      ],
      "metadata": {
        "id": "CG9f06-n90kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create essays that are difficult for AI detectors to flag as AI-generated, you can leverage the Speculative RAG approach with some real-world customization. Here's a step-by-step guide on how to apply this in a practical context:\n",
        "\n",
        "### 1. **Problem Formulation**\n",
        "   - **Goal**: Generate essays that blend high-quality, human-like content with factual accuracy while avoiding detection by AI detectors.\n",
        "   - **Approach**: Utilize Speculative RAG to draft multiple versions of essay paragraphs using different sources and perspectives, then refine and combine the best segments into a final essay.\n",
        "\n",
        "### 2. **Setting Up the Environment**\n",
        "   - Use the same setup as described earlier with the `drafter_model` and `verifier_model`. Additionally, include a style-checking model or heuristic to assess and modify the tone, structure, and language complexity.\n",
        "\n",
        "### 3. **Document Retrieval and Clustering**\n",
        "   - Retrieve diverse sets of documents related to the essay topic from reliable sources (e.g., academic papers, books, articles). Cluster these documents into different themes or perspectives.\n",
        "\n",
        "   ```python\n",
        "   essay_topic = \"The impact of climate change on global agriculture\"\n",
        "   retrieved_docs = retrieve_documents(essay_topic)  # Custom function to fetch relevant docs\n",
        "   doc_embeddings = embedding_model.encode(retrieved_docs)\n",
        "\n",
        "   # Cluster the documents into themes like \"economic impact\", \"environmental consequences\", \"policy changes\", etc.\n",
        "   num_clusters = 4\n",
        "   kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "   kmeans.fit(doc_embeddings)\n",
        "   clusters = kmeans.labels_\n",
        "\n",
        "   doc_subsets = []\n",
        "   for i in range(num_clusters):\n",
        "       cluster_docs = [retrieved_docs[j] for j in range(len(retrieved_docs)) if clusters[j] == i]\n",
        "       doc_subsets.append(cluster_docs)\n",
        "   ```\n",
        "\n",
        "### 4. **Draft Generation with a Human-like Style**\n",
        "   - Focus on generating content that mimics human writing styles, including varied sentence structures, nuanced language, and a mix of formal and informal tones.\n",
        "\n",
        "   ```python\n",
        "   drafts = []\n",
        "   for subset in doc_subsets:\n",
        "       input_text = \" \".join(subset)\n",
        "       inputs = drafter_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "       outputs = drafter_model.generate(**inputs)\n",
        "       draft = drafter_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "       \n",
        "       # Apply style modifications to make it more human-like\n",
        "       humanized_draft = enhance_human_style(draft)  # Custom function to tweak style\n",
        "       drafts.append(humanized_draft)\n",
        "   ```\n",
        "\n",
        "### 5. **Verification with Human-Like Consistency**\n",
        "   - Use the verifier model to evaluate each draft, focusing on human-like coherence, consistency, and originality. You might also integrate a paraphrasing step to further differentiate drafts.\n",
        "\n",
        "   ```python\n",
        "   best_draft = None\n",
        "   best_score = float('-inf')\n",
        "\n",
        "   for draft in drafts:\n",
        "       verifier_input = verifier_tokenizer(draft, return_tensors=\"pt\")\n",
        "       verifier_output = verifier_model.generate(**verifier_input)\n",
        "       score = verifier_model(**verifier_input, labels=verifier_input[\"input_ids\"]).loss.item()\n",
        "       \n",
        "       # Incorporate additional checks for human-like qualities\n",
        "       score += evaluate_human_likeness(draft)  # Custom function for evaluating style and originality\n",
        "       \n",
        "       if score > best_score:\n",
        "           best_draft = draft\n",
        "           best_score = score\n",
        "   ```\n",
        "\n",
        "### 6. **Final Assembly**\n",
        "   - Combine the best parts of each draft into a cohesive essay, ensuring variation in style and content to evade detection. Make manual tweaks if necessary to add unique insights or subtle errors that AI might miss.\n",
        "\n",
        "   ```python\n",
        "   final_essay = combine_drafts(drafts)  # Custom function to merge the best parts of drafts\n",
        "   print(\"Final Essay:\", final_essay)\n",
        "   ```\n",
        "\n",
        "### 7. **Testing Against AI Detectors**\n",
        "   - Before finalizing the essay, run it through various AI detection tools to identify and adjust any parts that might trigger flags. Use this feedback loop to refine your approach further.\n",
        "\n",
        "   ```python\n",
        "   detection_score = check_ai_detection(final_essay)  # Custom function to simulate AI detection checks\n",
        "   while detection_score > threshold:\n",
        "       final_essay = further_humanize(final_essay)  # Custom function for additional tweaks\n",
        "       detection_score = check_ai_detection(final_essay)\n",
        "   ```\n",
        "\n",
        "### 8. **Deploy and Iterate**\n",
        "   - Use this method to generate essays on various topics, iterating on the process and refining your models to consistently produce high-quality, undetectable content.\n",
        "\n",
        "### **Real-World Considerations**\n",
        "   - **Ethics**: Consider the ethical implications of generating undetectable AI-generated content, especially in academic and professional contexts.\n",
        "   - **Legal**: Be aware of potential legal ramifications of using AI to produce content that is intended to evade detection, particularly in situations where originality is paramount.\n",
        "\n",
        "This approach not only applies the Speculative RAG framework to a practical, real-world problem but also opens up avenues for more sophisticated content generation strategies."
      ],
      "metadata": {
        "id": "9X24xwa795Kz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pI62yULWs4fc",
        "outputId": "d03dbaec-73e1-431a-9247-95af936347ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers sentence-transformers scikit-learn openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LXjrY99ozZpU",
        "outputId": "b98c709b-116d-4d7f-d806-f1b7a2d244c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Retrieve the OpenAI API key stored as a secret\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Verify the key is loaded\n",
        "if openai.api_key is None:\n",
        "    raise ValueError(\"API key not found. Ensure OPENAI_API_KEY is set as a secret in Colab.\")\n",
        "\n",
        "# Function to fetch documents using the OpenAI API using chat completions\n",
        "def fetch_documents_from_openai(query):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Specify the chat model, like gpt-4\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "# Define research queries for testing\n",
        "queries = [\n",
        "    \"Provide an overview of recent research on the impact of climate change on global agriculture.\",\n",
        "    \"Summarize papers discussing the economic consequences of climate change.\",\n",
        "    \"Explain policies that address climate change's effect on agriculture.\"\n",
        "]\n",
        "\n",
        "# Fetch documents for each query\n",
        "retrieved_docs = [fetch_documents_from_openai(query) for query in queries]\n",
        "\n",
        "# Display the documents\n",
        "for idx, doc in enumerate(retrieved_docs):\n",
        "    print(f\"Document {idx+1}:\\n{doc}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSVJFRxOxNpb",
        "outputId": "a1311563-cc68-4043-e6fd-82725b4e205e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "Various studies conducted recently highlight the significant impact of climate change on global agriculture and food security. \n",
            "\n",
            "1. Yield Fluctuation: A study by Ray et al. (2019) indicates that climate change is responsible for an average yield reduction of 1%-2% per decade over the past century for the world's top crops such as maize, wheat, and barley. \n",
            "\n",
            "2. Impact on Nutrition: Climate change affects not only the quantity but also the quality of the crops. A research paper by Myers et al. (2014) found that rising CO2 levels are associated with reduced protein and micronutrient contents in key crops like rice and wheat, worsening nutritional security.\n",
            "\n",
            "3. Shift in Crop Patterns: Studies, such as those by Zhao et al. (2017) and Zabel et al. (2019), indicate that climate change is leading to a shift in agricultural zones, with warmer temperatures allowing farming in previously cold regions, while adversely affecting crop productivity in hotter climates. \n",
            "\n",
            "4. Increased Pest Threat: Another worrying consequence of climate change on agriculture is the expected increase in pest threats. A paper by Deutsch et al. (2018) predicts that global warming will likely expand the range of many crop pests, exacerbating the challenge of food security. \n",
            "\n",
            "5. Irregular Rainfall Patterns: Recent studies such as by Iizumi et al. (2018) and Pathak et al. (2019), highlight how climate change-induced alterations in rainfall patterns are affecting farming. More frequent droughts and floods can lead to major crop losses. \n",
            "\n",
            "6. Market Instability: Leclère et al. (2020) highlighted that since the local impacts of global warming vary greatly depending on the specific climatic changes and crops involved, significant global market instability is expected, influencing food prices and accessibility. \n",
            "\n",
            "The complexity of climate change effects on agriculture necessitates more research to develop adaptive measures, improve farming techniques, and secure food availability under changing climatic conditions.\n",
            "\n",
            "Document 2:\n",
            "1. \"Economic and policy consequences of climate change\" - This paper highlights the potential catastrophic economic impact of climate change. It focuses on the significant costs related to dealing with property damage, health issues, and food supply disruptions due to climate change. The authors suggest proactive policy-making, including carbon pricing and investments in green technologies.\n",
            "\n",
            "2. \"Climate change, economic growth, and inequality\" - The researchers in this study perform an analysis of the impact of warming temperatures on global economic growth and income inequality. They found that climate change would disproportionally affect poorer, hotter countries, exacerbating income inequality.\n",
            "\n",
            "3. \"Climate change in the US: The pros and cons of carbon markets\" - The paper discusses the economic impact of climate change on the United States economy and explores the effectiveness of carbon markets in mitigating these impacts. The research shows that carbon market could be part of the solution by reducing greenhouse gas emissions, but implementation is complex and requires sound regulation. \n",
            "\n",
            "4. \"Estimating Economic Damage from Climate Change in the United States\" - Researchers provide comprehensive estimates of the damage caused by climate change in the United States under different scenarios. The paper indicates that if no further action is taken, climate change could result in significant economic damages, particularly in areas such as agriculture, crime, and coastal storms.\n",
            "\n",
            "5. \"The Economic Effects of Climate Change\" - The authors report that climate change could potentially impact almost all sectors of the economy. It also brings up the aspect of \"climate adaptation\", where economies adjust to ongoing climate change, however, adaptation costs will also be a significant economic burden.\n",
            "\n",
            "6. \"Policy responses to climate change\" - This paper provides an economic analysis of policy choices for responding to climate change and argues that policies need to be flexible and adaptable. It concludes that all countries will need to undertake robust efforts on both mitigation and adaptation strategies to limit the economic consequences of climate change. \n",
            "\n",
            "7. \"The Economics of Climate Change Impacts and Policy Benefits at City Scale\" - This research estimates the economic impacts of climate change at the city level and the benefits of adaptation policies. It points out that urban adaptation strategies could bring significant economic benefits that outweigh the costs of implementing these policies.\n",
            "\n",
            "Document 3:\n",
            "Climate change significantly impacts agriculture through changes in temperature, precipitation patterns, and extreme weather events. Policies to address these issues often adopt various strategies, including promoting sustainable farming practices, investing in agricultural research and development, improving market accessibility, and providing financial assistance to farmers. \n",
            "\n",
            "1. Sustainable Agriculture Policies: These promote organic farming and the use of sustainable agricultural techniques such as agroforestry, aquaponics, permaculture, and conservation tillage. They encourage farmers to improve soil health, conserve water, decrease pesticide use, reduce greenhouse gas emissions and energy use, and enhance biodiversity.\n",
            "\n",
            "2. Agricultural R&D Policies: These promote research and development in agriculture to adapt to the impacts of climate change. The focus often involves breeding climate-resilient crop varieties, developing innovative irrigation technologies, creating efficient fertilizers, and implementing climate-smart agricultural practices.\n",
            "\n",
            "3. Insurance and Financial Assistance Policies: These provide financial protection to farmers affected by extreme weather events tied to climate change, such as droughts, floods, and heatwaves. Policies can offer crop insurance, disaster relief funds, and low-interest loans to help farmers recover from losses and invest in climate-resilient infrastructure.\n",
            "\n",
            "4. Market Accessibility Policies: These help farmers overcome challenges posed by climate change, such as food price volatility and supply chain disruptions. Policies can include developing infrastructure for transportation and storage, providing market information platforms, facilitating trade agreements, and implementing price stabilization measures.\n",
            "\n",
            "5. Carbon Market Policies: Some policies incentivize farmers to sequester carbon in their soils and forests and reduce greenhouse gas emissions. These include payments for ecosystem services, carbon credits or trading schemes, and subsidies for climate-smart agricultural practices.\n",
            "\n",
            "6. Food Security Policies: Lastly, considering that climate change could potentially jeopardize global food security, governments may look at food reserves, nutritional programs, and improving access to food for vulnerable communities.\n",
            "\n",
            "7. Education and Training Policies: These are aimed at providing farmers with the knowledge and skills they need to adapt to the adverse effects of climate change. This could include workshops on new farming techniques, or providing resources on how to make agricultural processes more sustainable.\n",
            "\n",
            "It's important to note that the most effective policies are usually context-sensitive, taking into account the local climate, geography, and agricultural practices.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load the drafter model (small specialist model)\n",
        "drafter_model_name = \"facebook/bart-large\"\n",
        "drafter_model = AutoModelForSeq2SeqLM.from_pretrained(drafter_model_name)\n",
        "drafter_tokenizer = AutoTokenizer.from_pretrained(drafter_model_name)\n",
        "\n",
        "# Load the verifier model (larger generalist model)\n",
        "verifier_model_name = \"google/t5-large-lm-adapt\"\n",
        "verifier_model = AutoModelForSeq2SeqLM.from_pretrained(verifier_model_name)\n",
        "verifier_tokenizer = AutoTokenizer.from_pretrained(verifier_model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUzwop5buj6D",
        "outputId": "d1101d00-a899-4e85-b0ee-a83c0e390c88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Assume `retrieved_docs` is a list of retrieved documents\n",
        "doc_embeddings = embedding_model.encode(retrieved_docs)\n",
        "\n",
        "# Cluster the documents\n",
        "# Change the number of clusters to be less than or equal to the number of documents\n",
        "num_clusters = min(3, len(retrieved_docs))  # This ensures num_clusters <= n_samples\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(doc_embeddings)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# Create subsets from each cluster\n",
        "doc_subsets = []\n",
        "for i in range(num_clusters):\n",
        "    cluster_docs = [retrieved_docs[j] for j in range(len(retrieved_docs)) if clusters[j] == i]\n",
        "    doc_subsets.append(cluster_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-QwNYD8ussU",
        "outputId": "10136a78-2cbe-4d3e-a895-7c205978b0fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drafts = []\n",
        "for subset in doc_subsets:\n",
        "    input_text = \" \".join(subset)\n",
        "    inputs = drafter_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = drafter_model.generate(**inputs)\n",
        "    draft = drafter_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    drafts.append(draft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezEV_9R_vGPD",
        "outputId": "c6d79f7d-33df-47aa-ede7-08f8681bca6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_draft = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for draft in drafts:\n",
        "    verifier_input = verifier_tokenizer(draft, return_tensors=\"pt\")\n",
        "    verifier_output = verifier_model.generate(**verifier_input)\n",
        "    score = verifier_model(**verifier_input, labels=verifier_input[\"input_ids\"]).loss.item()\n",
        "\n",
        "    if score > best_score:\n",
        "        best_draft = draft\n",
        "        best_score = score\n"
      ],
      "metadata": {
        "id": "vIAAkEnovdz5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Draft:\", best_draft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J7xssVuvhB2",
        "outputId": "42dc5f5b-0346-4373-bce3-b3d6ddbaadbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Draft: Climate change significantly impacts agriculture through changes in temperature, precipitation patterns, and extreme weather events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essay_topic = \"The impact of climate change on global agriculture\"\n",
        "retrieved_docs = fetch_documents_from_openai(essay_topic)  # Custom function to fetch relevant docs\n",
        "\n",
        "# Ensure retrieved_docs is a list of strings (documents)\n",
        "# If it's a single string, split it into sentences or paragraphs\n",
        "if isinstance(retrieved_docs, str):\n",
        "    # Example: Splitting into sentences using nltk\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    retrieved_docs = nltk.sent_tokenize(retrieved_docs)\n",
        "\n",
        "doc_embeddings = embedding_model.encode(retrieved_docs)\n",
        "\n",
        "# Cluster the documents into themes like \"economic impact\", \"environmental consequences\", \"policy changes\", etc.\n",
        "num_clusters = 4\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(doc_embeddings)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "doc_subsets = []\n",
        "for i in range(num_clusters):\n",
        "    cluster_docs = [retrieved_docs[j] for j in range(len(retrieved_docs)) if clusters[j] == i]\n",
        "    doc_subsets.append(cluster_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UciLqzPEvkCg",
        "outputId": "eca2b3a1-8762-45e2-f5b6-f1b4c92c70e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch synonyms using OpenAI's Chat API\n",
        "def get_synonyms(word):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Adjust depending on your access\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Provide synonyms for words.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"What are some synonyms for the word '{word}' that sound natural and human-like?\"}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    # Extracting text from the response\n",
        "    text = response.choices[0].message['content'].strip()\n",
        "    # Assume the response lists synonyms in a comma-separated format\n",
        "    synonyms = text.split(', ')\n",
        "    return synonyms\n"
      ],
      "metadata": {
        "id": "5xZnOkeO3OIQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def enhance_human_style(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    enhanced_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        enhanced_words = []\n",
        "\n",
        "        for word in words:\n",
        "            # Replace words with synonyms at random with a controlled probability\n",
        "            if random.random() < 0.3:  # 30% chance to replace a word with its synonym\n",
        "                synonyms = get_synonyms(word)\n",
        "                if synonyms:\n",
        "                    # Select a random synonym to replace the word\n",
        "                    enhanced_words.append(random.choice(synonyms))\n",
        "                else:\n",
        "                    enhanced_words.append(word)\n",
        "            else:\n",
        "                enhanced_words.append(word)\n",
        "\n",
        "        # Reconstruct the sentence\n",
        "        enhanced_sentence = ' '.join(enhanced_words)\n",
        "        enhanced_sentences.append(enhanced_sentence)\n",
        "\n",
        "    # Join all enhanced sentences into the final text\n",
        "    enhanced_text = ' '.join(enhanced_sentences)\n",
        "    return enhanced_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY1YAoFK3Th1",
        "outputId": "cf778cff-d759-462d-a241-a2cf0c01946b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drafts = []\n",
        "for subset in doc_subsets:\n",
        "    input_text = \" \".join(subset)\n",
        "    inputs = drafter_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = drafter_model.generate(**inputs)\n",
        "    draft = drafter_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Apply style modifications to make it more human-like\n",
        "    humanized_draft = enhance_human_style(draft)  # Custom function to tweak style\n",
        "    drafts.append(humanized_draft)"
      ],
      "metadata": {
        "id": "B8Xisvsvvpw_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_human_likeness(text):\n",
        "    # This function will use OpenAI's API to evaluate text\n",
        "    # Use openai.ChatCompletion.create for chat models like gpt-4\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # or \"gpt-4\" if you have access\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # system message\n",
        "            {\"role\": \"user\", \"content\": f\"Rate how natural and human-like the following text sounds: \\\"{text}\\\". Give a score from 0 to 10.\"}  # user message\n",
        "        ],\n",
        "        max_tokens=10,\n",
        "        temperature=0\n",
        "    )\n",
        "    # Extract the content from the message\n",
        "    response_text = response.choices[0].message['content'].strip()\n",
        "    # Find numeric values in the response\n",
        "    import re\n",
        "    match = re.search(r'\\d+', response_text)\n",
        "    if match:\n",
        "        score = float(match.group(0))\n",
        "    else:\n",
        "        score = 0  # Default score if no numbers are found\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "NRgmMBSW4L5T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_draft = None\n",
        "best_score = float('-inf')\n",
        "\n",
        "for draft in drafts:\n",
        "    verifier_input = verifier_tokenizer(draft, return_tensors=\"pt\")\n",
        "    verifier_output = verifier_model.generate(**verifier_input)\n",
        "    score = verifier_model(**verifier_input, labels=verifier_input[\"input_ids\"]).loss.item()\n",
        "\n",
        "    # Incorporate additional checks for human-like qualities\n",
        "    score += evaluate_human_likeness(draft)  # Custom function for evaluating style and originality\n",
        "\n",
        "    if score > best_score:\n",
        "        best_draft = draft\n",
        "        best_score = score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_mnJChMv2Pq",
        "outputId": "e4a910e8-fd8a-471e-d1df-60419a8522a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_drafts1(drafts):\n",
        "    \"\"\"\n",
        "    This function takes a list of drafts and combines them into a single final essay.\n",
        "    You can implement your logic here to select the best parts of each draft\n",
        "    or simply concatenate them together.\n",
        "    \"\"\"\n",
        "    # This is a simple example that just concatenates the drafts:\n",
        "    final_essay = \" \".join(drafts)\n",
        "\n",
        "    # More sophisticated logic can be added here to select and combine the best parts of each draft\n",
        "    # For instance, you could use sentence similarity, keyword extraction, etc.\n",
        "    # to identify and merge the most relevant content from each draft.\n",
        "\n",
        "    return final_essay"
      ],
      "metadata": {
        "id": "lHPg6NRN5yeF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_drafts(drafts):\n",
        "    \"\"\"\n",
        "    Use GPT-4 to combine multiple drafts into a coherent final essay.\n",
        "    \"\"\"\n",
        "    prompt = \"Combine the following drafts into a coherent essay: \" + \" BREAK \".join(drafts)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a skilled writer and editor.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1024,  # Adjust based on the length needed\n",
        "        temperature=0.5\n",
        "    )\n",
        "    final_essay = response.choices[0].message['content'].strip()\n",
        "    return final_essay\n"
      ],
      "metadata": {
        "id": "acWoJvPR6dXD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_essay = combine_drafts(drafts)  # Custom function to merge the best parts of drafts\n",
        "print(\"Final Essay:\", final_essay)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27rKURmqv22w",
        "outputId": "3165710f-14d4-4d46-8533-858e3cae2c96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Essay: Climate change has a profound influence on our world, first and foremost through alterations in temperature, changes in precipitation, and the absorption of carbon dioxide. This influence is not only significant but also multifaceted. One of the most prominent areas affected is global agriculture and food production, a sector that is highly sensitive to the environment.\n",
            "\n",
            "An increase in global temperature directly affects the science of plant biology, impacting the greenery on our planet. This rise in heat can result in a decrease in crop yields, posing a serious threat to food security worldwide. Additionally, alterations in precipitation patterns can cause droughts or floods, both of which are detrimental to agriculture.\n",
            "\n",
            "Moreover, the absorption of carbon dioxide plays a crucial role in this context. Plants absorb carbon dioxide for photosynthesis, a process fundamental to their growth. However, an excess of this gas due to human activities can lead to global warming, further exacerbating the challenges faced by agriculture.\n",
            "\n",
            "Given the severity of these impacts, it is imperative to devise and implement effective mitigation strategies. These strategies should aim at reducing greenhouse gas emissions, adopting sustainable farming practices, and improving the resilience of the agricultural sector to climate change. By doing so, we can safeguard our food supply and protect our planet for future generations.\n",
            "\n",
            "In conclusion, the influence of climate change on global agriculture is profound and multifaceted. Through an increase in temperature, changes in precipitation, and the absorption of carbon dioxide, climate change poses a serious threat to food production. However, with effective mitigation strategies, we can confront these challenges and ensure a sustainable future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def check_ai_detection(text):\n",
        "    \"\"\"\n",
        "    Use GPT-4 to evaluate if the text sounds like it was written by a human.\n",
        "    \"\"\"\n",
        "    prompt = f\"Does the following text sound like it was written by a human or generated by AI? Provide a likelihood score from 0 to 10: \\\"{text}\\\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI detection expert.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0\n",
        "    )\n",
        "    score_text = response.choices[0].message['content'].strip()\n",
        "    # Extract numerical score\n",
        "    match = re.search(r'\\d+', score_text)\n",
        "    score = float(match.group(0)) if match else 10  # Default to a high detection score if unclear\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "i-bfsHwi6zeL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def further_humanize(text):\n",
        "    # Example adjustments to humanize text\n",
        "    adjustments = [\n",
        "        \"Let's consider\", \"Interestingly enough,\", \"On the other hand,\"\n",
        "    ]\n",
        "    sentences = text.split(\". \")\n",
        "    enhanced_sentences = [f\"{random.choice(adjustments)} {sentence}\" if random.random() < 0.2 else sentence for sentence in sentences]\n",
        "    return \". \".join(enhanced_sentences)\n"
      ],
      "metadata": {
        "id": "RTXwp3ko7TB5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 5  # Set a threshold for the detection score\n",
        "final_essay = combine_drafts(drafts)\n",
        "\n",
        "detection_score = check_ai_detection(final_essay)\n",
        "while detection_score > threshold:\n",
        "    final_essay = \" \".join(further_humanize(final_essay))  # Implement further_humanize to adjust text style\n",
        "    detection_score = check_ai_detection(final_essay)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "collapsed": true,
        "id": "Av3SdXdR7N1m",
        "outputId": "951853fa-1c46-47a3-c832-69752d4fa9cc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Request too large for gpt-4 in organization org-eneUUpqPCDVGqgpMR0BmDusj on tokens per min (TPM): Limit 80000, Requested 119574. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f64e0dd05763>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mdetection_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal_essay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfurther_humanize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Implement further_humanize to adjust text style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdetection_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ai_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-f3462f79aa74>\u001b[0m in \u001b[0;36mcheck_ai_detection\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Does the following text sound like it was written by a human or generated by AI? Provide a likelihood score from 0 to 10: \\\"{text}\\\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Request too large for gpt-4 in organization org-eneUUpqPCDVGqgpMR0BmDusj on tokens per min (TPM): Limit 80000, Requested 119574. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detection_score = check_ai_detection(final_essay)  # Custom function to simulate AI detection checks\n",
        "while detection_score > threshold:\n",
        "    final_essay = further_humanize(final_essay)  # Custom function for additional tweaks\n",
        "    detection_score = check_ai_detection(final_essay)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "collapsed": true,
        "id": "Nv0WAI2Kv7iV",
        "outputId": "f1c17e26-19d6-4e70-cccb-6e6be3a24c32"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 're' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-5d97153221c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetection_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ai_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Custom function to simulate AI detection checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mdetection_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfinal_essay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfurther_humanize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Custom function for additional tweaks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdetection_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ai_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-526fa8d9f38a>\u001b[0m in \u001b[0;36mcheck_ai_detection\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscore_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Extract numerical score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Default to a high detection score if unclear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def check_ai_detection(text):\n",
        "    \"\"\"\n",
        "    Use GPT-4 to evaluate if the text sounds like it was written by a human.\n",
        "    Split text into smaller parts if necessary to stay within token limits.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"Does the following text sound like it was written by a human or generated by AI? \\\"{text}\\\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI detection expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0\n",
        "        )\n",
        "        score_text = response.choices[0].message['content'].strip()\n",
        "        match = re.search(r'\\d+', score_text)\n",
        "        score = float(match.group(0)) if match else 10  # Default to a high score if no numeric response\n",
        "        return score\n",
        "    except openai.RateLimitError:\n",
        "        print(\"Rate limit exceeded, waiting to retry...\")\n",
        "        time.sleep(60)  # Wait 60 seconds before retrying\n",
        "        return check_ai_detection(text)\n",
        "\n",
        "def further_humanize(text):\n",
        "    # Apply local transformations without needing additional API calls\n",
        "    adjustments = [\"Indeed,\", \"Furthermore,\", \"Moreover,\", \"However,\"]\n",
        "    sentences = text.split(\". \")\n",
        "    enhanced_sentences = [f\"{random.choice(adjustments)} {sentence}\" if random.random() < 0.2 else sentence for sentence in sentences]\n",
        "    return \". \".join(enhanced_sentences).replace(\" .\", \".\")\n",
        "\n",
        "# Example usage\n",
        "final_essay = combine_drafts(drafts)  # Assuming combine_drafts is optimized for token usage\n",
        "detection_score = check_ai_detection(final_essay)\n",
        "while detection_score > threshold:\n",
        "    final_essay = further_humanize(final_essay)  # Use local changes to reduce API calls\n",
        "    detection_score = check_ai_detection(final_essay)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "collapsed": true,
        "id": "YOHEJzTxv8FN",
        "outputId": "33c7ac45-2879-4e1c-d9c4-290dd03b5d78"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'RateLimitError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0b440f5db65e>\u001b[0m in \u001b[0;36mcheck_ai_detection\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Does the following text sound like it was written by a human or generated by AI? \\\"{text}\\\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0b440f5db65e>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mdetection_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mfinal_essay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfurther_humanize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use local changes to reduce API calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdetection_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ai_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_essay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-0b440f5db65e>\u001b[0m in \u001b[0;36mcheck_ai_detection\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Default to a high score if no numeric response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRateLimitError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit exceeded, waiting to retry...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait 60 seconds before retrying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'RateLimitError'"
          ]
        }
      ]
    }
  ]
}